{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-12T01:27:43.940605Z",
     "start_time": "2026-02-12T01:27:43.259543Z"
    }
   },
   "source": [
    "import dbof.dataset_creation.zarr_dataset as zarr_dataset\n",
    "import dbof.io.filesystems as filesystems"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T01:27:45.026666Z",
     "start_time": "2026-02-12T01:27:43.941843Z"
    }
   },
   "cell_type": "code",
   "source": "import src.dataloader.cutout_torch_dataloader as cutout_torch_dataloader",
   "id": "5d09057b2e35ea35",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T01:27:45.068607Z",
     "start_time": "2026-02-12T01:27:45.066104Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "b81d3fddb9ab42c6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T01:27:45.786009Z",
     "start_time": "2026-02-12T01:27:45.070118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bucket = \"dbof\" #data_cfg[\"bucket\"]\n",
    "folder = \"native_grid_dbof_training_data\"\n",
    "s3_endpoint = \"https://s3-west.nrp-nautilus.io\"\n",
    "feature_channels = ['Eta', 'Salt', 'Theta', 'U', 'V', 'W', 'relative_vorticity', 'log_gradb']\n",
    "run_id = \"big_run_00\"\n",
    "\n",
    "fs, fs_synch = filesystems.create_s3_filesystems(s3_endpoint)\n",
    "\n",
    "reader = zarr_dataset.ZarrDatasetReader(\n",
    "    bucket=bucket,\n",
    "    folder=folder,\n",
    "    run_id=run_id,\n",
    "    dataset_name=\"dataset_creation.zarr\",\n",
    "    fs=fs\n",
    ")"
   ],
   "id": "e13de871de19fc06",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T01:31:38.363667Z",
     "start_time": "2026-02-12T01:27:45.789520Z"
    }
   },
   "cell_type": "code",
   "source": "loader = cutout_torch_dataloader.make_dbof_cutout_dataloader(reader, subset=10240, batch_size=64, num_workers=0, transform=None)",
   "id": "e16885b6dd01a03d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jake Tallman\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 57040 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Images into memory (10240, 8, 64, 64) ...\n",
      "Loading ids into memory (10240,) ...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# todo get cuda working",
   "id": "a914292d7e0590f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T01:44:37.898055Z",
     "start_time": "2026-02-12T01:44:30.696485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dino = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")  # small\n",
    "dino.eval().cuda()"
   ],
   "id": "71d6618c0513daf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to C:\\Users\\Jake Tallman/.cache\\torch\\hub\\main.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jake Tallman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\Jake Tallman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\Jake Tallman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to C:\\Users\\Jake Tallman/.cache\\torch\\hub\\checkpoints\\dinov2_vits14_pretrain.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84.2M/84.2M [00:03<00:00, 24.8MB/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m dino = torch.hub.load(\u001B[33m\"\u001B[39m\u001B[33mfacebookresearch/dinov2\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mdinov2_vits14\u001B[39m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# small\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mdino\u001B[49m\u001B[43m.\u001B[49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1082\u001B[39m, in \u001B[36mModule.cuda\u001B[39m\u001B[34m(self, device)\u001B[39m\n\u001B[32m   1065\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] = \u001B[38;5;28;01mNone\u001B[39;00m) -> Self:\n\u001B[32m   1066\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[32m   1067\u001B[39m \n\u001B[32m   1068\u001B[39m \u001B[33;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1080\u001B[39m \u001B[33;03m        Module: self\u001B[39;00m\n\u001B[32m   1081\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1082\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    927\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    931\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    932\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    933\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    938\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    939\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:955\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    951\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    952\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m955\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    956\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    958\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1082\u001B[39m, in \u001B[36mModule.cuda.<locals>.<lambda>\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1065\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] = \u001B[38;5;28;01mNone\u001B[39;00m) -> Self:\n\u001B[32m   1066\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[32m   1067\u001B[39m \n\u001B[32m   1068\u001B[39m \u001B[33;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1080\u001B[39m \u001B[33;03m        Module: self\u001B[39;00m\n\u001B[32m   1081\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1082\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DBOF_Representation_Learning\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:403\u001B[39m, in \u001B[36m_lazy_init\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    398\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    399\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    400\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmultiprocessing, you must use the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mspawn\u001B[39m\u001B[33m'\u001B[39m\u001B[33m start method\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    401\u001B[39m     )\n\u001B[32m    402\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch._C, \u001B[33m\"\u001B[39m\u001B[33m_cuda_getDeviceCount\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mTorch not compiled with CUDA enabled\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    404\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[32m    406\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    407\u001B[39m     )\n",
      "\u001B[31mAssertionError\u001B[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
